:PROPERTIES:
:ID:       cfbcb4c2-c850-4676-9922-3987c92486bf
:END:
#+title: Parallel_computing

L'idée est de diviser une tache en sous tache et d'allouer ces taches à différents ordinateurs.

Le compromis est toujours de savoir si le coûts d'allocation ne dépasse pas le gain à diviser la tache.

** les paquets permettant de la faire:

*** Python:

- Pool
- dask

*** R

- parallel

** Des frameworks

*** Apache Hadoop

- HDFS : c'est un système de distribution de fichiers pour de multiple machines

  Il est maintenant souvent remplacer par des solutions spécifiques à chaque plate-forme

- Map Reduce: On divise les opérations et on les répartie en plusieurs machines. C'était compliqué et donc une des réponses a été de développer Hive et un SQL associé.

*** Apache Spark

Fonctionne sur Resilient distributed datasets (RDD)

On put y effectuer deux types d'opérations:

    - Des transformations comme ~.map()~ ou ~.filter()~
    - Des actins comme ~.count()~ ou ~.first()~


On utilise Spark à travers une API comme [[id:ea521b0a-287f-439d-aef6-e27244d1cfe7][PySpark]]. Il y a aussi une abstraction pour les tableaux de données.
