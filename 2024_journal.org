#+title: Journal 2024 #+author: Olivier leroy


* <2024-01-01 Mon>

Nouvelle annee, resolution tenir le journal jusqu'au bout et transformer certains partie en memo (blog post?)

- ne pas distinguer le journal du taff du perso
- pas encore certains d'utiliser org roam + un journal a voir

** org mod :org:mode:

    Utiliser plus souvent les tags ref: https://orgmode.org/guide/Tags.html

** building repro analytical pipeline :rstats:

*** chapitre 4:

Pedagogique, sans doute tres utile pour un etudiant et/ou un jeune DA

Chose nouvelle apprise: ~git stash~

Permet de revenir as ~HEAD~

#+begin_src bash
mkdir temp/
cd temp
git init
echo "bob" > unfichier.txt
git add .
git commit -m "init"
rm unfichier.txt
git add .
git stash
cd ..
rm -rf temp
#+end_src

#+RESULTS:
| Reinitialized | existing | Git       | repository | in    | /Users/olivierleroy/Documents/oli/prise_de_notes/temp/.git/ |     |    |         |         |      |
| On            | branch   | master    |            |       |                                                           |     |    |         |         |      |
| nothing       | to       | commit,   | working    | tree  | clean                                                     |     |    |         |         |      |
| Saved         | working  | directory | and        | index | state                                                     | WIP | on | master: | 5c5c45b | init |

*trunk base development* :  https://trunkbaseddevelopment.com/

On vera sans doute plus tard mais en gros on dev sur dev et fork des releases qui elles sont stables.

* <2024-01-02 Tue>

** chapitre 5 reproducible pip R :rstats:

trunk based dev: en gros des commits tres frequents, faciliter par de la double revision. L'objectif est de diviser le development en petite taches permettant de petite commit, dev sur une autre branche et merger rapidement.

Dans le cas de taches importantes il est recommande d'utiliser une technique appelle "branch bt abstraction" qui consiste a utiliser des placeholders qui seront ameliore petit a petit. (https://martinfowler.com/bliki/BranchByAbstraction.html)

Le tronc est sense etre toujours "production rdy"

l'obligation de sousmettre une PR pourmodifier le tronc renforce cette pratique

*** BEAD :mbtiles:PG:

pas d option de filter sur un array dans mbtiles

bcp de wrangling pour former un jeux de donnees test, pas mal de changement de specs,

les fonctions d'aggregation de PG peuvent prendre un order by et un distinct:

~json_agg('isp_id' || ' ' || isp_id order by isp_id) as isp_num~

** TED

vue metadata avec Dolley

* <2024-01-03 Wed>

** bead :PG:

~ctid~ est une colonne utile pour avoir un id pour des jointures, elle reference la place en memoire de l info

https://www.postgresql.org/docs/current/ddl-system-columns.html

*** RDOF

targets a un format url pour suivre l update d'un fichier en ligne cf ~format = "url"~

https://www.fcc.gov/document/fcc-proposes-over-8m-fines-against-22-rdof-applicants-defaults

** general

penser a organiser un meeting ou on fixe les donnees, leur types, leur noms

* <2024-01-04 Thu>

** bead

0 for null in isp_id for every count

pas mal de test avec le types pour les mbtiles fixe la structure de donnees

*** pg stuff :PG:

replace() a un equivalent  REGEXP_replace(isp_idv2, '{|}', '', 'g')  avec un argument pour faire une regex global

todo demain clean le mess d acs pipeline


** jour poses

01/01/2024
05/27/2024
06/14/2024
06/19/2024
09/01/2024
11/28/2024
11/29/2024
12/25/2024

** chapitre 6 Functional programming

Utilisation de fonction d'ordre superieur

on a un premier passage qui indique ce qu'est l'etat a travers un exemple de ce qui est en memoire. L'etat ne devrait pas changer le resultats d'une fonction.

Certains fonctions peuvent incrementer ou changer un etat dans le programe. c'est le cas par exemple d'une boucle. Je suis pas fan de l'argument de deinir la list dans comme argument.

Si une fonction est pure elle doit ne prendre en compte que des arguments dans ses arguments et pas dans d'autre environement.

#+begin_src R
y <- 10
bad <- function(x) {
       x + y
}
# un mauvais example
#+end_src


Dans R les fonction peuvent aussi etre des arguments

#+begin_src R
g <- function(number, f, ...) {
  f(number, ...)
    }
#+end_src

ici l'ellipsis permet de passer des argument pris en compte par f, l'exmple typique est ~na.rm = TRUE~.

Programmer avec cela en tete indique qu'il est sage de mettre des gardes fou a ses functions, l'exemple utilise est ~sqrt(-5)~ qui ne produit que un warnings et ne retourne pas un nombre mais NaN.

 Un autre exmemple que l'auteur utilise est un wrapper generique autour de ~trycatch()~ qui permet de faire une fonction qui retourne un warnings en erreur.

 Cela permet aussi d'introduire les function factories: ie des fonctions qui vont retourner des fonctions.

 Les arguments par defaut sont rapdiement introduits.

 Assertive/safe programming est aussi introduit. Ce n'est pas toujours limpide pour moi jusqu'ou tester.

 l'aurteur deconseil aussi d'utiliser des fonctions recursives. Aucune idee de pkoi mon intuition est que R est pas fait pour garder en memoire (ici dans l env de la fonction) le resultat de la fonction precedente.

 L'auteur se reclame aussi de la penser unix pour demander une decomposition en petite fonction. Je suis pas expert.

 IL y a un gros passage sur l'interet des listes. Cela lui permet d'introduire Reduce et Map. Il y aussi un gros passage sur les lists dans les colonnes et comment cela permet de faire des graphiques parametres.

 S3, purrr et withr sont introduit succintement.

* <2024-01-05 Fri>

jour de nettoyage, bead et TED pipeline

bcp a faire sur BEAD, car c'est un mixe de sql lance via pg admin et de targets

je ne suis pas claire sur ce que fait crew, est une interface derriere plusieurs moteur ou un moteur sur une interface specifique

* <2024-01-07 Sun>

** R stuff :rstats:

~getS3method("function", "class")~ permet de lister les methodes

~sf:::print.sgbp~  marche aussi

* <2024-01-08 Mon>

** TED

pas mal de pb avec la pipeine, merged le bazard et reussi a mettre a jour, pas vraiment appris grand chose si ce n'est que data table n'est multi thread que si ~libomp~ est disponible.

** BEAD

pas mal d'explo avec isp la version json introduit des ~"~ ce qui change l'ordre du classement mais ne devrais pas changer le nombre ..

* <2024-01-09 Tue>


flood

** bead

essaie de regrouper en une seule pileine BEAD stuff

** <2024-01-10 Wed>

pas de meetng et discussion sur l'archivage, l'utilisation du cache

nettoyage de certains repos et merge de branche

un peu de test d sqlite avec gpkg

* <2024-01-11 Thu>

mon plan isp et rdof

** rdof

fork it and just do one for RDOF

verifications des fichier downloades

*** R stuff :rstats:
se rapeller

#+begin_src R
test <- system2("ls", "xx",
    stdout = TRUE, stderr = TRUE)

attr(test, "status")
# va retourner 1
#+end_src

pratique pour tester des lignes de commandes

Pour sauver une targets dans mermaid:

#+begin_src R
mm <- tar_mermaid()
writeLines(mm, "tar
    get_mermaid.js")

#+end_src


* <2024-01-12 Fri>

** metadata

petit bilan de metadata et test de {dm} pour representer un schema

** BEAD

    isp / rdof

* <2024-01-15 Mon>


** BEAD

rdof premiere version pour authorized + test versus previous data

* <2024-01-16 Tue>

Do a bit of stuff for Camden

** BEAD

finalement compris ready to authorized / authorized / et default category

mis en place default mais difference avec previous version

** book club :rstats:

~methods(class = "table")~

* <2024-01-17 Wed>


** BEAD

simplification de la logique de rdof entre auth , les deux versions de default

postgresql has des operations de set https://www.postgresql.org/docs/current/queries-union.html


* <2024-01-18 Thu>

** BEAD

on a bouger enfin sur rdof

technology flags, geoid bl x tech x new alias

je me suis rapeller que j'ai le tab 10 20 du census pour la mise en relation

** job

https://join.tts.gsa.gov/resume/

* <2024-01-20 Sat>

je rejoute le vendredi passe:

aws fait payer ses instances T sur la base de nano puis applique un multiplicateur en fonctiom.
Par exemple on multiplie par 32 pour passer a "large"

update de mon pgpass pro

Il est possible de passer des var de psql via sql et de le faire de make via shell

lower + raise + j switch back to qwerty

* <2024-01-22 Mon>

** BEAD

isp cleaning and docs

advetised speed upload and download max by technology / provider

has_award_geoid_bl doit etre false if null (pareil for has_technology)

0 can be "not reported"

combo requriement and day to day; how do we solve this issue

giving agenda, project leadership, provide more feedback


* <2024-01-23 Tue>

** BEAD

pas mal de mise en place pour l'echantillon


* <2024-01-24 Wed>

copy crosswalk to bead schema

dm materialise les relations one to many -> one en surlignant le nom de la colonne.


* <2024-01-25 Thu>

penser a organiser les ticket / tasks

#+begin_src R
 is_ogr2ogr_here <- function() {
    ogr_version <- system2("ogr2ogr", "--version",
                           stdout = TRUE, stderr = TRUE)
    if (! is.null(attr(ogr_version, "status"))) stop("Is ogr2ogr installed?")
    return(0)
  }
#+end_src

* <2024-01-26 Fri>

on mange la pilllule et on test la methode ntia

pas mal de temp s sur git: il faut toujoirs penser etre sur un graph

eu  un pb avec targets impossible a debugger, un reset hard sur la tete a resolu le pb

* <2024-01-29 Mon>


** bead

assez chiand de refaire le calcul d egibilite mais je peux simplifier certaines requetes sql.

AK legend

250010102062053

toujours verifier ses count(*), cela compte les lignes
* <2024-01-30 Tue>

we want :
  - we keep cnt_100_20
  - same for cnt_25_3
  - cnt_100_20 cable / fiber
  - underserved (25/3 > undersved < 100/20) cable / fiber
  - underserved dsl
  - undersedved fixed wireless



we want:
   - bl_100_20_area_cable_fiber

     bl_area_25_3_area_cable_fiber
      - 50/10 DSL <- does need to be counted here

   - bead_category_cable_fiber

* <2024-01-31 Wed>


pas mal de rush pour essayer d'ajouter un filtre underserved,

eda pour l'explorer, debut de rassemblement de

* <2024-02-01 Thu>

push to mapbox, me suis fait avoir par un shadow update de fcc

* <2024-02-02 Fri>

https://usbuildingdata.blob.core.windows.net/usbuildings-v2/Alabama.geojson.zip

* <2024-02-11 Sun>

un peu en retard pour la semaine.

j'ai pas mal tester jq (stream et non stream) et gnu parallel.

il me faut un model pour tester l'imprecision des buildings / fcc

commencer les routes par miles

ogr tricK : ~ogrinfo -al -so my.shp~

* <2024-02-12 Mon>

** BEAD

- rebuild with new requirements

- vu avec britany comment produire les tableau pour usda

- build ms for all geoid bl

** nouvelle machine

- [x] brew
- [x] slack/gh/google
- [x] iterm
- [x] R and VScode (+ radian, httpgd, languageserver)
- [x] emacs + doom
- [x] PG + postgis

* <2024-02-13 Tue>

commande pratique sur mac avec brew:

- brew list et brew cask --cask

* <2024-02-14 Wed>

acs

dot provides density

coloring block by nb of locations underserved / unserved

show only at specific scale / visual

road data: scale less useful at block but better at bigger aera

"premises per miles" / how you display that?

- what roads

- 500 feets,

** other scale/spatial join:

aggregate of premises count


* <2024-02-15 Thu>

pas mal de jq / bb calcul pour leur blog

* <2024-02-16 Fri>

* <2024-02-20 Tue>

** bead :PG:

st_memunion est une version moins gourmande en memoire que st_union

* <2024-02-21 Wed>

eligigibility -> bead_category

#+begin_src R
m <- mirai::mirai({
      library(nanonext)
      rep <- socket("rep", listen = "tcp://127.0.0.1:6556")
      reply(context(rep), execute = Sys.getpid, send_mode = "raw")
    })
#+end_src

https://beej.us/guide/bgnet0/html/split/networking-overview.html slowly learning

- pas mal de tiles pour BEAD

* <2024-02-22 Thu>

#+begin_src bash
docker run --rm -v $PWD:/tmp ghcr.io/degauss-org/geocoder:3.3.0 my_address_file.csv
#+end_src

- bcp de debugging, bcp trop de cc dans nos pipelines

* <2024-02-23 Fri>

Make automatic variable:

- ~$@~ the target of current rule

- ~$^~ dependecies

pas mal de pb avec TED, il faut revoir ce sschema

* <2024-02-26 Mon>

** bead

debut de rapatriement de rdof in the bead repo

modification du script pour dl un pgkg

meeting data

ajout de count loc pour aggregats tr / eligibility

* <2024-02-27 Tue>

** bead

recalcul d'un eligibility tr mais avec un dsl excluded

** calix

survey/outreach ntca spead capability / video services / cables

community based. edging network to other community

small coop/co -> telephone -> moved to broadband

education to community

VA services

850 isp

* <2024-02-28 Wed>

** bead

pas mal de tuiles de generees

dans certains cas desactive la lecture en parallel pour tippecanoe permet de faire passer un oom

ogrinfo. marche sur mbtiles et donne pas mal d'infos utiles


* <2024-02-29 Thu>

370810152013009

pct served 3.8

pct un and underseved 96.2 (should be 3/53)

filtered out previous version of rdof

* DONE <2024-03-04 Mon>

pour differencier rapidement 2 sets entre deux tables: - 1 pour le premier set

- 2 pour le second set

- les valeurs possiblent sont donc 1 (premier set), 2 (second set) et 3 (combinaisons)

* <2024-03-05 Tue>

010539698011035

pas mal de tickets pour les tests

 truc a voir en NC:

 - great dismal swamp

 - see atlas obscura

* <2024-03-07 Thu>

** bead

fcc funding map

* <2024-03-08 Fri>

pas mal de chose interesante du cote de test:

- https://github.com/yihui/testit

- https://github.com/brodieG/aammrtf

* <2024-03-11 Mon>

Passer 15 h de la semaine sur https://edzer.github.io/sswr/

Le reste: un petit peu de calix

toujours beaucoup de BEAD en partie sur funding broadband.

je galere pas mal sur le code de Brittany et j'aurais du mieux l'auditer.

* <2024-03-15 Fri>

- null or 0

- rename the cnt at block levl : _uncovered

- cnt_total_locations_uncovered (write the def somewhere)

* <2024-03-18 Mon>

pas mal de reunion CaLix, bead.

cote R je suis pas certains d'avoir le meme  comportement entre paste et sprint f avec un quote identifier/litteral

* <2024-03-19 Tue>

#+begin_src R
gimme_tidy_ilecs <- function(bl, ilecs) {
  sf::sf_use_s2(FALSE)
  sgbp <- sf::st_intersects(bl, ilecs)
  # need sf loaded I never know how to load s3
  df <- as.data.frame(sgbp)
  df$geoid_bl <- bl[["geoid_bl"]][df$row.id]
  df$id_ilecs <- ilecs[["id_ilecs"]][df$col.id]
  df_slim <- df[, c("geoid_bl", "id_ilecs")]
  return(df_slim)
}

# require bl_gpkg and ilecs
# number of core is hard coded, should it go in some param files?
# User should know their system and use the correct number -> readme
US_ilecs <- function(path_gpkg, ilecs) {

  list_st <- sf::st_read(path_gpkg,
                         query = "select geoid_st
                                  from sql_statement
                                  group by geoid_st")

  gimme_a_state <- function(a_state) {
    state <- sf::st_read(path_gpkg,
                         query = toString(
                                          sprintf(
                                  "select * from sql_statement
                                  where geoid_st ='%s'", a_state)))
    small_dat <- gimme_tidy_ilecs(state, ilecs)
    print(a_state)
    return(small_dat)
  }

  big_list <- parallel::mclapply(list_st$geoid_st, gimme_a_state, mc.cores = 4)

  dat <- do.call(rbind, big_list)
  return(dat)
}

#+end_src


NTIA will annouce bead award in Q3

** R postgres stuff

https://cran.r-project.org/web/packages/DBI/vignettes/DBI-advanced.html

on peut diviser un requetes de type dbGetQuery en plusieurs etape.

- envoie de la requete ou du statement (dbSendQuery ou dbSendStatement): va executer la commande
- dbFtech() va recuperer le resultat
- dbClearResult() libere les ressources et cloture la transaction.

*  <2024-03-20 Wed>

pas mal de reunion


<2024-03-21 Thu>

geoid_st
geod_co unique on county ->

rebuild bfm_award_proj_co

build temp table at loca with only column that matter to us

if it C for wired and wiredlfw remove it

* <2024-03-25 Mon>

pas mal de tickets

restructuration de pas mal de truc pour BEAD

* <2024-03-26 Tue>

bon repo sur l'encoding dans R: https://github.com/gaborcsardi/rencfaq

package pour des metrics de pkgs:
https://docs.ropensci.org/pkgstats/index.html

** some set up

 -  I like htop, tree, jq, gdal (obv git)

 - on vscode I like httpgd / radian

* <2024-03-27 Wed>


pas mal de maj pour BEAD

tester targets sur deux ordi, c'est pas fou vu que cela fait creer des conflits dans git avec la partie meta/.

DBI avec postgresql n aime pas les multi statement

* <2024-04-01 Mon>

- bead reorg

- blog post

- setup quarto

* <2024-04-02 Tue>

- nettoyer son pc c'est l'enfer

- nettoyage des tables et schema pas utilise

* <2024-04-03 Wed>

- ne pas oublier d'ajouter psql et plus dans le PATH

- creer .pgpass

- renomer pas mal de script bead

- aider dolley sur fcc pour calix

 - meeting jira et agile

*  <2024-04-04 Thu>

*  <2024-04-11 Thu>

toujours des probleme pour integrer les ISP

rajout de BIP et tribal pour le BEAD, calix coverage devrait etre simplifier et prise en compte des county

<2024-04-18 Thu>

si j'installe pas org je suis en retard dans ma prise de note.

pas mal de temps de perdu sur le connecticut:

le guide dev de l'api du census est tres bien, je dois aussi progresser un peu sur le l'utilisation de lapply et passer plus d'argument as une fonction.

je dois penser a mettre a jour mon linkedin

- j'ai travaille dans l'ecosystem aws: rds, ec2, s3
- developpe des pipelines pour des apps, programes de recherches et analyse de donnees
- type de donnees, US census, FCC broadband
- fixer pas mal de choses / misc geocoding

  fixer les donnees ISP

  documenter les dl.

  clean calix data

*  <2024-04-24 Wed>

 add cnt_bl unique

 add count of county

 do a group by frn and brand name (get an array of provider id)

 wiki -> isp

 * <2024-04-25 Thu>

 table(us county names)

 https://proximityone.com/tracts_evolution.htm

 https://www.census.gov/history/www/programs/geography/tracts_and_block_numbering_areas.html

 * <2024-04-26 Fri>

 - idea of blog / post / map : census county with same / different name

 - evolution of census tract over time

* <2024-05-02 Thu>


new grant for NSF

focus on states that do not get much research  funded

You have funding to support your research infrastucture (first step) that can lead to second step

Lisa works in HR on grant writing

Few colleges in VT can apply for this grant

Building a research ecosysten in rural ecosystem. Lead us to workforce dev. / Higher Ed  STEM. Degree -> pathway job

Landmark specific for student with disability (ADHD, Neuro diversity).

Training on research and writing grant for staff

Keeping the student we train here / connection with employer

What kind of skills they need for pathway job / "what is needed in the community"

4 years grant

"send our graduate"

CCV: Community College of Vermont

funding can only go for some specific states (ESCOR F4? states)

helping us and communicate of what we are doing

make connection between research and bussiness

ESCOR: tip of iceberg for more grant

January feb 2025 to get funding

** duckdb and polar

Hive style partitions -> in parquet format (parquet integrate it well but it could works with other file format, text one like csv)

Grant recommand to use {rspm}

if data bigger than 50gb : spark / map reduce infro more

(goal is kind of avoiding spark)

Also some team are polyglot

big data is more a question of scaling

arrow and duckdb play well together

https://onlinelibrary.wiley.com/doi/10.1002/pds.5728

duckdb has in memory modul diff than arrow, if you use parquet you can use either duck db / arrow or polars

*** Two revolutions:

- Better on disk storage : parquet metaphore is a floor cabinet

- Better in memory representation

  standardisation around apache arrow: format, columnar, allow zero copy

  both polars and duck db are OLAP (run natively does not run on server), run on defered materialisation rather eagerly

  we can be "lazy" and optimise queries before executing them

  explain verb is nice

  for very large data set using some sample for decile/median (because sort is always costly)

  with dplyr you need to convert it to an object that dplyr can read (tbl)

  ~show_query~ display the query run

  we need to use a ~collect~ method to run the query

  Polars also build on arrow but different query engine

  Both polars and duck db interface well

  tidypolars seems quite nice

** Agile learning

- kanbam  diff than Scrum meme si les deux vont utiliser un "board"

* <2024-05-06 Mon>

- calix Amanda category

- S3 explorations -> tidy wiki ? blog post / targets

- SDK AWS ?

  - FRN / check what that mean and if we can use it -> set up a meeting with Myles

* <2024-05-27 Mon>

** duckDB

a une implementation similaire a PF pour le catalog;

        - information_schema.schemata: Database, Catalog and Schema
        - information_schema.tables: Tables and Views
        - information_schema.columns: Columns




* <2024-07-15 Mon>

- besoin de comprendre si moins de combo isp affect les id_isp


- tract level of percentage of location that do not have 100/20
- county ->
- place - > blocl to place xwalk

  demain, nettoyer isp pour reessayer en partant de raw

* <2024-07-17 Wed>

Il me faut donner un shell a cori.data.fcc

* <2024-07-19 Fri>

Nora: works on desks / conferences

AK:
        - more resilent team / deeper bench
        - more competition
Drew:
        - lack of leadership
        - AWS /mapbox
        - monday / jira
        - research

Trish:
        - research
        - TTT
        - dev our risi fed grant / can we get fed dollar to explore our missions ?
        - Is data important geting fed funding
* <2024-07-22 Mon>

ask about vintage food

to run on DB: https://www.cybertec-postgresql.com/en/get-rid-of-your-unused-indexes/


* <2024-07-24 Wed>

https://www.r-bloggers.com/2017/06/how-to-add-code-coverage-codecov-to-your-r-package/


* <2024-07-26 Fri>

"static code analysis".


* <2024-08-01 Thu>

intersting read: https://cran.r-project.org/doc/FAQ/R-FAQ.html#How-can-I-turn-a-string-into-a-variable_003f-1 about get and assign

* <2024-08-06 Tue>

GH opermet de creer un template directory w/o the history of the template


Bridge Day - https://officialbridgeday.com/ People bungee jump off of the new River Gorge Bridge. In Fayetteville, WV which is an awesome town. Also the newest National Park.

Cranberry Glades Botanical area (Boardwalk, but be bear aware)

Falls of Hills Creek - hike to three waterfalls, Pocahontas County

Highland Scenic Highway - Pocahontas County - beautiful drive through national forest. Great in the fall, lots of hikes

BearTown State Park https://wvstateparks.com/park/beartown-state-park/ Boardwalk through moss covered rock formations.

SnowShoe Village- https://www.snowshoemtn.com/  great in the summer

Elk Springs Resort  https://elkspringswv.com/ Fly fishing and great food.

Mothman Statue in Point Pleasant - speaks for itself.

Cass Scenic Railroad - train ride up the mountain from an old coal town.

Hawks Nest State Park - ride tram cars into the bottom of the New River Gorge

Clarksburg/Bridgeport area - excellent Italian food from about everywhere.

Hillbilly Hot Dogs near Huntington - outrageous and worth a google.

Trans-Allegheny Lunatic Asylum - https://trans-alleghenylunaticasylum.com/ Horrifying.

Pepperoni Rolls!!

 * <2024-08-12 Mon>

What do we want with DB, read, compute

We will focus on DBMS and flat file

set of concepts:

relationship / dat integrity / data redundancy / data control

use of as.tible to get a summary


use of ~r"()"~ to allow "" in string

~dbGetQuery(con, sql, params)~ is very nice params take a list

for query the duckDB syntax also works: "from my_table" is a shortcut of "select * from my table"

~collect()~ as an argumentg n that by default is set to ~inf~

you can use head also or slice_sample if you want. sample

show_query() is very nice also

see with slice_sample

analytic = dbplyr then in production rewrite it in sql.

order of dplyr is important to generatye sql


check duckplyr package: duckdplyr df to parquet

check parquetize but mayeb not

check truncate

CORA plenty db to check

write in DB is a pain to write abstraction from R because you have a lot of different ways than a db engine can handle it.
