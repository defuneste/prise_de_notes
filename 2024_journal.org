#+title: Journal 2024 #+author: Olivier leroy


* <2024-01-01 Mon>

Nouvelle annee, resolution tenir le journal jusqu'au bout et transformer certains partie en memo (blog post?)

- ne pas distinguer le journal du taff du perso
- pas encore certains d'utiliser org roam + un journal a voir

** org mod :org:mode:

    Utiliser plus souvent les tags ref: https://orgmode.org/guide/Tags.html

** building repro analytical pipeline :rstats:

*** chapitre 4:

Pedagogique, sans doute tres utile pour un etudiant et/ou un jeune DA

Chose nouvelle apprise: ~git stash~

Permet de revenir as ~HEAD~

#+begin_src bash
mkdir temp/
cd temp
git init
echo "bob" > unfichier.txt
git add .
git commit -m "init"
rm unfichier.txt
git add .
git stash
cd ..
rm -rf temp
#+end_src

#+RESULTS:
| Reinitialized | existing | Git       | repository | in    | /Users/olivierleroy/Documents/oli/prise_de_notes/temp/.git/ |     |    |         |         |      |
| On            | branch   | master    |            |       |                                                           |     |    |         |         |      |
| nothing       | to       | commit,   | working    | tree  | clean                                                     |     |    |         |         |      |
| Saved         | working  | directory | and        | index | state                                                     | WIP | on | master: | 5c5c45b | init |

*trunk base development* :  https://trunkbaseddevelopment.com/

On vera sans doute plus tard mais en gros on dev sur dev et fork des releases qui elles sont stables.

* <2024-01-02 Tue>

** chapitre 5 reproducible pip R :rstats:

trunk based dev: en gros des commits tres frequents, faciliter par de la double revision. L'objectif est de diviser le development en petite taches permettant de petite commit, dev sur une autre branche et merger rapidement.

Dans le cas de taches importantes il est recommande d'utiliser une technique appelle "branch bt abstraction" qui consiste a utiliser des placeholders qui seront ameliore petit a petit. (https://martinfowler.com/bliki/BranchByAbstraction.html)

Le tronc est sense etre toujours "production rdy"

l'obligation de sousmettre une PR pourmodifier le tronc renforce cette pratique

*** BEAD :mbtiles:PG:

pas d option de filter sur un array dans mbtiles

bcp de wrangling pour former un jeux de donnees test, pas mal de changement de specs,

les fonctions d'aggregation de PG peuvent prendre un order by et un distinct:

~json_agg('isp_id' || ' ' || isp_id order by isp_id) as isp_num~

** TED

vue metadata avec Dolley

* <2024-01-03 Wed>

** bead :PG:

~ctid~ est une colonne utile pour avoir un id pour des jointures, elle reference la place en memoire de l info

https://www.postgresql.org/docs/current/ddl-system-columns.html

*** RDOF

targets a un format url pour suivre l update d'un fichier en ligne cf ~format = "url"~

https://www.fcc.gov/document/fcc-proposes-over-8m-fines-against-22-rdof-applicants-defaults

** general

penser a organiser un meeting ou on fixe les donnees, leur types, leur noms

* <2024-01-04 Thu>

** bead

0 for null in isp_id for every count

pas mal de test avec le types pour les mbtiles fixe la structure de donnees

*** pg stuff :PG:

replace() a un equivalent  REGEXP_replace(isp_idv2, '{|}', '', 'g')  avec un argument pour faire une regex global

todo demain clean le mess d acs pipeline


** jour poses

01/01/2024
05/27/2024
06/14/2024
06/19/2024
09/01/2024
11/28/2024
11/29/2024
12/25/2024

** chapitre 6 Functional programming

Utilisation de fonction d'ordre superieur

on a un premier passage qui indique ce qu'est l'etat a travers un exemple de ce qui est en memoire. L'etat ne devrait pas changer le resultats d'une fonction.

Certains fonctions peuvent incrementer ou changer un etat dans le programe. c'est le cas par exemple d'une boucle. Je suis pas fan de l'argument de deinir la list dans comme argument.

Si une fonction est pure elle doit ne prendre en compte que des arguments dans ses arguments et pas dans d'autre environement.

#+begin_src R
y <- 10
bad <- function(x) {
       x + y
}
# un mauvais example
#+end_src


Dans R les fonction peuvent aussi etre des arguments

#+begin_src R
g <- function(number, f, ...) {
  f(number, ...)
    }
#+end_src

ici l'ellipsis permet de passer des argument pris en compte par f, l'exmple typique est ~na.rm = TRUE~.

Programmer avec cela en tete indique qu'il est sage de mettre des gardes fou a ses functions, l'exemple utilise est ~sqrt(-5)~ qui ne produit que un warnings et ne retourne pas un nombre mais NaN.

 Un autre exmemple que l'auteur utilise est un wrapper generique autour de ~trycatch()~ qui permet de faire une fonction qui retourne un warnings en erreur.

 Cela permet aussi d'introduire les function factories: ie des fonctions qui vont retourner des fonctions.

 Les arguments par defaut sont rapdiement introduits.

 Assertive/safe programming est aussi introduit. Ce n'est pas toujours limpide pour moi jusqu'ou tester.

 l'aurteur deconseil aussi d'utiliser des fonctions recursives. Aucune idee de pkoi mon intuition est que R est pas fait pour garder en memoire (ici dans l env de la fonction) le resultat de la fonction precedente.

 L'auteur se reclame aussi de la penser unix pour demander une decomposition en petite fonction. Je suis pas expert.

 IL y a un gros passage sur l'interet des listes. Cela lui permet d'introduire Reduce et Map. Il y aussi un gros passage sur les lists dans les colonnes et comment cela permet de faire des graphiques parametres.

 S3, purrr et withr sont introduit succintement.

* <2024-01-05 Fri>

jour de nettoyage, bead et TED pipeline

bcp a faire sur BEAD, car c'est un mixe de sql lance via pg admin et de targets

je ne suis pas claire sur ce que fait crew, est une interface derriere plusieurs moteur ou un moteur sur une interface specifique

* <2024-01-07 Sun>

** R stuff :rstats:

~getS3method("function", "class")~ permet de lister les methodes

~sf:::print.sgbp~  marche aussi

* <2024-01-08 Mon>

** TED

pas mal de pb avec la pipeine, merged le bazard et reussi a mettre a jour, pas vraiment appris grand chose si ce n'est que data table n'est multi thread que si ~libomp~ est disponible.

** BEAD

pas mal d'explo avec isp la version json introduit des ~"~ ce qui change l'ordre du classement mais ne devrais pas changer le nombre ..

* <2024-01-09 Tue>


flood

** bead

essaie de regrouper en une seule pileine BEAD stuff

** <2024-01-10 Wed>

pas de meetng et discussion sur l'archivage, l'utilisation du cache

nettoyage de certains repos et merge de branche

un peu de test d sqlite avec gpkg

* <2024-01-11 Thu>

mon plan isp et rdof

** rdof

fork it and just do one for RDOF

verifications des fichier downloades

*** R stuff :rstats:
se rapeller

#+begin_src R
test <- system2("ls", "xx",
    stdout = TRUE, stderr = TRUE)

attr(test, "status")
# va retourner 1
#+end_src

pratique pour tester des lignes de commandes

Pour sauver une targets dans mermaid:

#+begin_src R
mm <- tar_mermaid()
writeLines(mm, "tar
    get_mermaid.js")

#+end_src


* <2024-01-12 Fri>

** metadata

petit bilan de metadata et test de {dm} pour representer un schema

** BEAD

    isp / rdof

* <2024-01-15 Mon>


** BEAD

rdof premiere version pour authorized + test versus previous data

* <2024-01-16 Tue>

Do a bit of stuff for Camden

** BEAD

finalement compris ready to authorized / authorized / et default category

mis en place default mais difference avec previous version

** book club :rstats:

~methods(class = "table")~

* <2024-01-17 Wed>


** BEAD

simplification de la logique de rdof entre auth , les deux versions de default

postgresql has des operations de set https://www.postgresql.org/docs/current/queries-union.html


* <2024-01-18 Thu>

** BEAD

on a bouger enfin sur rdof

technology flags, geoid bl x tech x new alias

je me suis rapeller que j'ai le tab 10 20 du census pour la mise en relation

** job

https://join.tts.gsa.gov/resume/

* <2024-01-20 Sat>

je rejoute le vendredi passe:

aws fait payer ses instances T sur la base de nano puis applique un multiplicateur en fonctiom.
Par exemple on multiplie par 32 pour passer a "large"

update de mon pgpass pro

Il est possible de passer des var de psql via sql et de le faire de make via shell

lower + raise + j switch back to qwerty

* <2024-01-22 Mon>

** BEAD

isp cleaning and docs

advetised speed upload and download max by technology / provider

has_award_geoid_bl doit etre false if null (pareil for has_technology)

0 can be "not reported"

combo requriement and day to day; how do we solve this issue

giving agenda, project leadership, provide more feedback


* <2024-01-23 Tue>

** BEAD

pas mal de mise en place pour l'echantillon


* <2024-01-24 Wed>

copy crosswalk to bead schema

dm materialise les relations one to many -> one en surlignant le nom de la colonne.


* <2024-01-25 Thu>

penser a organiser les ticket / tasks

#+begin_src R
 is_ogr2ogr_here <- function() {
    ogr_version <- system2("ogr2ogr", "--version",
                           stdout = TRUE, stderr = TRUE)
    if (! is.null(attr(ogr_version, "status"))) stop("Is ogr2ogr installed?")
    return(0)
  }
#+end_src

* <2024-01-26 Fri>

on mange la pilllule et on test la methode ntia

pas mal de temp s sur git: il faut toujoirs penser etre sur un graph

eu  un pb avec targets impossible a debugger, un reset hard sur la tete a resolu le pb

* <2024-01-29 Mon>


** bead

assez chiand de refaire le calcul d egibilite mais je peux simplifier certaines requetes sql.

AK legend

250010102062053

toujours verifier ses count(*), cela compte les lignes
* <2024-01-30 Tue>

we want :
  - we keep cnt_100_20
  - same for cnt_25_3
  - cnt_100_20 cable / fiber
  - underserved (25/3 > undersved < 100/20) cable / fiber
  - underserved dsl
  - undersedved fixed wireless



we want:
   - bl_100_20_area_cable_fiber

     bl_area_25_3_area_cable_fiber
      - 50/10 DSL <- does need to be counted here

   - bead_category_cable_fiber

* <2024-01-31 Wed>


pas mal de rush pour essayer d'ajouter un filtre underserved,

eda pour l'explorer, debut de rassemblement de

* <2024-02-01 Thu>

push to mapbox, me suis fait avoir par un shadow update de fcc

* <2024-02-02 Fri>

https://usbuildingdata.blob.core.windows.net/usbuildings-v2/Alabama.geojson.zip

* <2024-02-11 Sun>

un peu en retard pour la semaine.

j'ai pas mal tester jq (stream et non stream) et gnu parallel.

il me faut un model pour tester l'imprecision des buildings / fcc

commencer les routes par miles

ogr tricK : ~ogrinfo -al -so my.shp~

* <2024-02-12 Mon>

** BEAD

- rebuild with new requirements

- vu avec britany comment produire les tableau pour usda

- build ms for all geoid bl

** nouvelle machine

- [x] brew
- [x] slack/gh/google
- [x] iterm
- [x] R and VScode (+ radian, httpgd, languageserver)
- [x] emacs + doom
- [x] PG + postgis

* <2024-02-13 Tue>

commande pratique sur mac avec brew:

- brew list et brew cask --cask

* <2024-02-14 Wed>

acs

dot provides density

coloring block by nb of locations underserved / unserved

show only at specific scale / visual

road data: scale less useful at block but better at bigger aera

"premises per miles" / how you display that?

- what roads

- 500 feets,

** other scale/spatial join:

aggregate of premises count


* <2024-02-15 Thu>

pas mal de jq / bb calcul pour leur blog

* <2024-02-16 Fri>

* <2024-02-20 Tue>

** bead :PG:

st_memunion est une version moins gourmande en memoire que st_union

* <2024-02-21 Wed>

eligigibility -> bead_category

#+begin_src R
m <- mirai::mirai({
      library(nanonext)
      rep <- socket("rep", listen = "tcp://127.0.0.1:6556")
      reply(context(rep), execute = Sys.getpid, send_mode = "raw")
    })
#+end_src

https://beej.us/guide/bgnet0/html/split/networking-overview.html slowly learning

- pas mal de tiles pour BEAD

* <2024-02-22 Thu>

#+begin_src bash
docker run --rm -v $PWD:/tmp ghcr.io/degauss-org/geocoder:3.3.0 my_address_file.csv
#+end_src

- bcp de debugging, bcp trop de cc dans nos pipelines

* <2024-02-23 Fri>

Make automatic variable:

- ~$@~ the target of current rule

- ~$^~ dependecies

pas mal de pb avec TED, il faut revoir ce sschema

* <2024-02-26 Mon>

** bead

debut de rapatriement de rdof in the bead repo

modification du script pour dl un pgkg

meeting data

ajout de count loc pour aggregats tr / eligibility

* <2024-02-27 Tue>

** bead

recalcul d'un eligibility tr mais avec un dsl excluded

** calix

survey/outreach ntca spead capability / video services / cables

community based. edging network to other community

small coop/co -> telephone -> moved to broadband

education to community

VA services

850 isp

* <2024-02-28 Wed>

** bead

pas mal de tuiles de generees

dans certains cas desactive la lecture en parallel pour tippecanoe permet de faire passer un oom

ogrinfo. marche sur mbtiles et donne pas mal d'infos utiles


* <2024-02-29 Thu>

370810152013009

pct served 3.8

pct un and underseved 96.2 (should be 3/53)

filtered out previous version of rdof

* DONE <2024-03-04 Mon>

pour differencier rapidement 2 sets entre deux tables: - 1 pour le premier set

- 2 pour le second set

- les valeurs possiblent sont donc 1 (premier set), 2 (second set) et 3 (combinaisons)

* <2024-03-05 Tue>

010539698011035

pas mal de tickets pour les tests

 truc a voir en NC:

 - great dismal swamp

 - see atlas obscura

* <2024-03-07 Thu>

** bead

fcc funding map

* <2024-03-08 Fri>

pas mal de chose interesante du cote de test:

- https://github.com/yihui/testit

- https://github.com/brodieG/aammrtf

* <2024-03-11 Mon>

Passer 15 h de la semaine sur https://edzer.github.io/sswr/

Le reste: un petit peu de calix

toujours beaucoup de BEAD en partie sur funding broadband.

je galere pas mal sur le code de Brittany et j'aurais du mieux l'auditer.

* <2024-03-15 Fri>

- null or 0

- rename the cnt at block levl : _uncovered

- cnt_total_locations_uncovered (write the def somewhere)

* <2024-03-18 Mon>

pas mal de reunion CaLix, bead.

cote R je suis pas certains d'avoir le meme  comportement entre paste et sprint f avec un quote identifier/litteral

* <2024-03-19 Tue>

#+begin_src R
gimme_tidy_ilecs <- function(bl, ilecs) {
  sf::sf_use_s2(FALSE)
  sgbp <- sf::st_intersects(bl, ilecs)
  # need sf loaded I never know how to load s3
  df <- as.data.frame(sgbp)
  df$geoid_bl <- bl[["geoid_bl"]][df$row.id]
  df$id_ilecs <- ilecs[["id_ilecs"]][df$col.id]
  df_slim <- df[, c("geoid_bl", "id_ilecs")]
  return(df_slim)
}

# require bl_gpkg and ilecs
# number of core is hard coded, should it go in some param files?
# User should know their system and use the correct number -> readme
US_ilecs <- function(path_gpkg, ilecs) {

  list_st <- sf::st_read(path_gpkg,
                         query = "select geoid_st
                                  from sql_statement
                                  group by geoid_st")

  gimme_a_state <- function(a_state) {
    state <- sf::st_read(path_gpkg,
                         query = toString(
                                          sprintf(
                                  "select * from sql_statement
                                  where geoid_st ='%s'", a_state)))
    small_dat <- gimme_tidy_ilecs(state, ilecs)
    print(a_state)
    return(small_dat)
  }

  big_list <- parallel::mclapply(list_st$geoid_st, gimme_a_state, mc.cores = 4)

  dat <- do.call(rbind, big_list)
  return(dat)
}

#+end_src


NTIA will annouce bead award in Q3

** R postgres stuff

https://cran.r-project.org/web/packages/DBI/vignettes/DBI-advanced.html

on peut diviser un requetes de type dbGetQuery en plusieurs etape.

- envoie de la requete ou du statement (dbSendQuery ou dbSendStatement): va executer la commande
- dbFtech() va recuperer le resultat
- dbClearResult() libere les ressources et cloture la transaction.

*  <2024-03-20 Wed>

pas mal de reunion


<2024-03-21 Thu>

geoid_st
geod_co unique on county ->

rebuild bfm_award_proj_co

build temp table at loca with only column that matter to us

if it C for wired and wiredlfw remove it

* <2024-03-25 Mon>

pas mal de tickets

restructuration de pas mal de truc pour BEAD

* <2024-03-26 Tue>

bon repo sur l'encoding dans R: https://github.com/gaborcsardi/rencfaq

package pour des metrics de pkgs:
https://docs.ropensci.org/pkgstats/index.html

** some set up

 -  I like htop, tree, jq, gdal (obv git)

 - on vscode I like httpgd / radian

* <2024-03-27 Wed>


pas mal de maj pour BEAD

tester targets sur deux ordi, c'est pas fou vu que cela fait creer des conflits dans git avec la partie meta/.

DBI avec postgresql n aime pas les multi statement

* <2024-04-01 Mon>

- bead reorg

- blog post

- setup quarto

* <2024-04-02 Tue>

- nettoyer son pc c'est l'enfer

- nettoyage des tables et schema pas utilise

* <2024-04-03 Wed>

- ne pas oublier d'ajouter psql et plus dans le PATH

- creer .pgpass

- renomer pas mal de script bead

- aider dolley sur fcc pour calix

 - meeting jira et agile

*  <2024-04-04 Thu>

*  <2024-04-11 Thu>

toujours des probleme pour integrer les ISP

rajout de BIP et tribal pour le BEAD, calix coverage devrait etre simplifier et prise en compte des county

<2024-04-18 Thu>

si j'installe pas org je suis en retard dans ma prise de note.

pas mal de temps de perdu sur le connecticut:

le guide dev de l'api du census est tres bien, je dois aussi progresser un peu sur le l'utilisation de lapply et passer plus d'argument as une fonction.

je dois penser a mettre a jour mon linkedin

- j'ai travaille dans l'ecosystem aws: rds, ec2, s3
- developpe des pipelines pour des apps, programes de recherches et analyse de donnees
- type de donnees, US census, FCC broadband
- fixer pas mal de choses / misc geocoding

  fixer les donnees ISP

  documenter les dl.

  clean calix data

*  <2024-04-24 Wed>

 add cnt_bl unique

 add count of county

 do a group by frn and brand name (get an array of provider id)

 wiki -> isp

 * <2024-04-25 Thu>

 table(us county names)

 https://proximityone.com/tracts_evolution.htm

 https://www.census.gov/history/www/programs/geography/tracts_and_block_numbering_areas.html

 * <2024-04-26 Fri>

 - idea of blog / post / map : census county with same / different name

 - evolution of census tract over time

* <2024-05-02 Thu>


new grant for NSF

focus on states that do not get much research  funded

You have funding to support your research infrastucture (first step) that can lead to second step

Lisa works in HR on grant writing

Few colleges in VT can apply for this grant

Building a research ecosysten in rural ecosystem. Lead us to workforce dev. / Higher Ed  STEM. Degree -> pathway job

Landmark specific for student with disability (ADHD, Neuro diversity).

Training on research and writing grant for staff

Keeping the student we train here / connection with employer

What kind of skills they need for pathway job / "what is needed in the community"

4 years grant

"send our graduate"

CCV: Community College of Vermont

funding can only go for some specific states (ESCOR F4? states)

helping us and communicate of what we are doing

make connection between research and bussiness

ESCOR: tip of iceberg for more grant

January feb 2025 to get funding

** duckdb and polar

Hive style partitions -> in parquet format (parquet integrate it well but it could works with other file format, text one like csv)

Grant recommand to use {rspm}

if data bigger than 50gb : spark / map reduce infro more

(goal is kind of avoiding spark)

Also some team are polyglot

big data is more a question of scaling

arrow and duckdb play well together

https://onlinelibrary.wiley.com/doi/10.1002/pds.5728

duckdb has in memory modul diff than arrow, if you use parquet you can use either duck db / arrow or polars

*** Two revolutions:

- Better on disk storage : parquet metaphore is a floor cabinet

- Better in memory representation

  standardisation around apache arrow: format, columnar, allow zero copy

  both polars and duck db are OLAP (run natively does not run on server), run on defered materialisation rather eagerly

  we can be "lazy" and optimise queries before executing them

  explain verb is nice

  for very large data set using some sample for decile/median (because sort is always costly)

  with dplyr you need to convert it to an object that dplyr can read (tbl)

  ~show_query~ display the query run

  we need to use a ~collect~ method to run the query

  Polars also build on arrow but different query engine

  Both polars and duck db interface well

  tidypolars seems quite nice

** Agile learning

- kanbam  diff than Scrum meme si les deux vont utiliser un "board"

* <2024-05-06 Mon>

- calix Amanda category

- S3 explorations -> tidy wiki ? blog post / targets

- SDK AWS ?

  - FRN / check what that mean and if we can use it -> set up a meeting with Myles

* <2024-05-27 Mon>

** duckDB

a une implementation similaire a PF pour le catalog;

        - information_schema.schemata: Database, Catalog and Schema
        - information_schema.tables: Tables and Views
        - information_schema.columns: Columns




* <2024-07-15 Mon>

- besoin de comprendre si moins de combo isp affect les id_isp


- tract level of percentage of location that do not have 100/20
- county ->
- place - > blocl to place xwalk

  demain, nettoyer isp pour reessayer en partant de raw

* <2024-07-17 Wed>

Il me faut donner un shell a cori.data.fcc

* <2024-07-19 Fri>

Nora: works on desks / conferences

AK:
        - more resilent team / deeper bench
        - more competition
Drew:
        - lack of leadership
        - AWS /mapbox
        - monday / jira
        - research

Trish:
        - research
        - TTT
        - dev our risi fed grant / can we get fed dollar to explore our missions ?
        - Is data important geting fed funding
* <2024-07-22 Mon>

ask about vintage food

to run on DB: https://www.cybertec-postgresql.com/en/get-rid-of-your-unused-indexes/


* <2024-07-24 Wed>

https://www.r-bloggers.com/2017/06/how-to-add-code-coverage-codecov-to-your-r-package/


* <2024-07-26 Fri>

"static code analysis".


* <2024-08-01 Thu>

intersting read: https://cran.r-project.org/doc/FAQ/R-FAQ.html#How-can-I-turn-a-string-into-a-variable_003f-1 about get and assign

* <2024-08-06 Tue>

GH opermet de creer un template directory w/o the history of the template


Bridge Day - https://officialbridgeday.com/ People bungee jump off of the new River Gorge Bridge. In Fayetteville, WV which is an awesome town. Also the newest National Park.

Cranberry Glades Botanical area (Boardwalk, but be bear aware)

Falls of Hills Creek - hike to three waterfalls, Pocahontas County

Highland Scenic Highway - Pocahontas County - beautiful drive through national forest. Great in the fall, lots of hikes

BearTown State Park https://wvstateparks.com/park/beartown-state-park/ Boardwalk through moss covered rock formations.

SnowShoe Village- https://www.snowshoemtn.com/  great in the summer

Elk Springs Resort  https://elkspringswv.com/ Fly fishing and great food.

Mothman Statue in Point Pleasant - speaks for itself.

Cass Scenic Railroad - train ride up the mountain from an old coal town.

Hawks Nest State Park - ride tram cars into the bottom of the New River Gorge

Clarksburg/Bridgeport area - excellent Italian food from about everywhere.

Hillbilly Hot Dogs near Huntington - outrageous and worth a google.

Trans-Allegheny Lunatic Asylum - https://trans-alleghenylunaticasylum.com/ Horrifying.

Pepperoni Rolls!!

 * <2024-08-12 Mon>

What do we want with DB, read, compute

We will focus on DBMS and flat file

set of concepts:

relationship / dat integrity / data redundancy / data control

use of as.tible to get a summary


use of ~r"()"~ to allow "" in string

~dbGetQuery(con, sql, params)~ is very nice params take a list

for query the duckDB syntax also works: "from my_table" is a shortcut of "select * from my table"

~collect()~ as an argumentg n that by default is set to ~inf~

you can use head also or slice_sample if you want. sample

show_query() is very nice also

see with slice_sample

analytic = dbplyr then in production rewrite it in sql.

order of dplyr is important to generatye sql


check duckplyr package: duckdplyr df to parquet

check parquetize but mayeb not

check truncate

CORA plenty db to check

write in DB is a pain to write abstraction from R because you have a lot of different ways than a db engine can handle it.

Comparisons are important every regression is a comparison

odds are hard to interpret

$$ odds = p / 1 -p $$

quality of comparison depend on from who and how

check marginaleffects

marginals effect take a function. a model and var

use a comparison as a function

marginal effect also as a pot wrapper, that return a gg plot object

https://marginaleffects.com/

---


provides more that just the prediction: uncertainty usefull also to inform the bound they want to operate on

confidence intervals / prediction intervals

first one uncertainty in fitting model

second uncertainity in sample

one is about the world the other one is about a specific object

conformal prediction (here in a regression context)

you can calibrate/adjust bounds

A gentle intro do conformal prediction

Keras 3

"The Bitter Lesson"

deep learning become more popular because compute become available

supposed to be versatile

forgiving and reliable

Keras is like a marble maze, composing a pth to data to go throught

keras come with plenty of layers, metrics, loss functions, optimizers, etc

one example is build a classifier

compile - > fit

example give yuy 92,1 % accuracy

keras help you test with more features

progressive disclosure of complexity

keras is implemented with reticulate

reticulate: updated Python environement

Kera 3 return of multi backend support

keras not part of tensor flow now and support multiple backend

and can switch between back end

that is helping the deployement

All function are pure, no side effect

plenty of other stuffs

https://keras3.posit.co/

deepr learning with R

"deep learning is like duck tape"

---

how to avoid garbage in garbage out

in pharma the used to classify some images

a experiment can have 40K at minumum, hence how to you do automate QC

utilisation of isolationforest

how many image are getting flags: depend on threshold

---

R

Collin gillespie

- csv: it just works, it just works next year

  in memory only

  larger file: in memory imposible

  parquet store metadata

  #+begin_src R
c(4,4,4,4,4,1,2,2,2,2,2)
  #+end_src


parquet use some trick to repeated idea rep 4, 5 times


Dictionary encoding

rds and parquet hace different compression (gzip/snappy)

snappy a bit faster on reading. buit cost a bit much on compression


R foundation: individual membership 25E

example of package:

rvertnet: API for natural history

datapackgeR: automated creation and update of R package

---

tyler morgan:

use quarto render hooks ! in the quarto yaml

delivery for email with quarto: pins / quarto / posit connect


quarto as email format

---

genAI

"guilty pleasure"

being a fan is always wanting more?

gpt 4.0 do text image and table

available on free tier

convert image is nice

prompt enginiring is going hand in hand with analsys skill

prompt need to be specific

---

open data


set and tools of process to answer question

technology trigger for data science: eniacs

none of the problem have been solved and we need a set of tools to solve

use resampling (bootstrap resampling)

model of population -> simulated data

peak DS 2009 - 2012

trough of desillution: 2015 -2018

data journalism is improving data litteracy

the general social survey

---

Data engineering

Reclaiming your weeck end with data contract

Trust is everything

How much control do you have in my outputs/inputs

Reliability / consistency / autonomy

solution data contract: an agreement, organization dynamics

what make a good data contract:

1. people:
        * Owner
        * Responsible
        * Consulted

2. Expectetion
        * Schema
        * Valid values
        * SLA, update/frequency
        * Interface, where is teh data stored, need on query perfomance
        * Compliance
        * Interoperaibility, can it be reads by machine/peole

 Who builds the contracts:
       - data consummer and producer

 Technology: helpful but not everything

literrate contratcs

 npelikan/positconf2024-datacontracts

Lead with empathy, learn their pain point, sometimes it can be you

keep it simple, how much validation do I need


** Data modeling for data engineer

Turn vague terms into something operational

Divide the logic

customers with ltv < 30 no recent order since 30 days

can we document and test this workflow for the rest of team?

Customers are dimensional model of a "customer"
Maintained by growth & pricing

Columns:
- description and constraints

-> allow to write test

example: write / audit / publish

Develop/test and document /deploy

standardization is an important part of a data engineers job

what is data modeling:

Joe Resis definition:

> a structured representation that organizes standardizes data to enavle and guide humand and machine behavior, inform decision making and faciliate actions

why it's important

It helps you scale, use a DAG

how to get started

data modeling is a process:

- start small, start documenting pain point
- keep iterating
- have fun, it is a process and you should have fun

It is tool agnostic

** targets / mirai

async frameworks

what is (next generation) async ?

Async is a first class experience in many languages, async is not waiting when parallelism is happening

parallel is parallel not async: bringing async

NNG implements async in C

nanonext brings nng to R

-> connect thousands of parralell process

A promising object -> a promise can be used in shiny

new kind of promisses: event-driven: no polling & zero latency

in crew they are controller and workers (process)

mirai manage task and crew

Create a controller (~crew_controller_local~)

controller has methods example push submit and experssion

Crew can pluging in AWS batch/slurm

parrallel + async in shiny is very nice

see targets crew vignettes


--

baby steps to production

online deployments categories:

- general deployement
- interactive notebooks with sharing functionnality
- code-first data science outputs

 scalability / security

 rapid prototyping

 posit connect is free from gh repos

*  <2024-08-16 Fri>

- ajouter une favicon a mon site

  Paul bunyan

  THREE RIVER TELCO

  BLACKFOOT TELEPHONE COOPERATIVE

  BULLOCH TELEPHONE COOPERATIVE

* <2024-08-20 Tue>

~psql -E~ to test

https://github.com/ruralinnovation/calix_data_analysis_proj/blob/main/P0303/02.2_assemble_risi_vars_SERVER.R

https://github.com/ruralinnovation/georgia_state_plan/blob/main/01_a_task_served_unserved_bl.R


adding the citation

* <2024-09-02 Mon>

faire un boiler plate repo avec github et super simple
a rajouter a un bragging doc

* <2024-09-09 Mon>

#+begin_src R

get_table <- function(schema){
  force(schema)
  function(table_name) {
    con <- cori.db::connect_to_db(schema)
    on.exit(DBI::dbDisconnect(con))
    DBI::dbReadTable(con, table_name)
  }
}

get_proj_bead <- get_table("proj_bead")
get_proj_bead("isp_name_v4")

write_to_schema <- function(schema) {
  force(schema)
  function(data, table_name) {
    con <- cori.db::connect_to_db(schema)
    on.exit(DBI::dbDisconnect(con), add = TRUE)
    message(schema)
    (DBI::dbWriteTable(conn = con,
                       name = table_name,
                       value = data,
                       overwrite = TRUE))
  }
}

#+end_src


* <2024-09-17 Tue>

duckdb ne peut pas executer plus d'un alter statement

ie alter my table add columns bob, add column bill; ne vas pas marcher.

aja: 3 A:
- authentification
- authorization
- access control

  <2024-09-19 Thu>

  parquet for relatin table?

  function to wrap dbExecute et aide a utiliser le retour en fonction de ce qui est execute



* <2024-09-20 Fri>

https://engelsjk.com/posts/the-dark-side-of-zip-codes/

** ZIP: Zone improvement plan
        - 1 national area ex: 9 calif /orgegon / wash
        - 2 state
        - 3 sectional center
        - 4 et 5 Small post office or zone
* <2024-10-11 Fri>

#+begin_src scheme
(define atom?
        (lambda (x)
          (and (not (pair? x)) (not (null? x)))))
#+end_src


* <2024-10-28 Mon>

Need to check National Standard Codes maintained by USGS

good list of ANSI code: https://www.census.gov/library/reference/code-lists/ansi.html

Functional Status Codes (FUNCSTAT): relate to type of gov.

FIPS Class Codes (CLASSFP): what are they related to FIPS
